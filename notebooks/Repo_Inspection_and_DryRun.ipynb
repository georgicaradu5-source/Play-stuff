{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e6d1ad",
   "metadata": {},
   "source": [
    "> Note: Before running the Metrics & Coverage cell, generate fresh coverage artifacts.\n",
    ">\n",
    "> - Preferred: `nox -s test`\n",
    "> - Or: `pytest --cov=src --cov-report=xml`\n",
    ">\n",
    "> The metrics step expects `coverage.xml` at the repository root. If missing, run one of the above first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21075e32",
   "metadata": {},
   "source": [
    "# X Agent Unified Repository Inspection & Dry-Run Notebook\n",
    "\n",
    "This notebook performs a structured, offline inspection and smoke validation of the Play-stuff (Unified X Agent) repository.\n",
    "\n",
    "Outline Covered:\n",
    "1. Setup (env + deps)\n",
    "2. GitHub tree fetch (optional offline if internet disabled) / local file enumeration\n",
    "3. Parse Python modules (AST) & build dependency graph\n",
    "4. ASCII + formatting checks\n",
    "5. Config schema validation\n",
    "6. Telemetry init (noop vs enabled)\n",
    "7. SQLite storage initialization & inspection\n",
    "8. BudgetManager simulation\n",
    "9. RateLimiter simulation\n",
    "10. XClient dry-run stubbed calls\n",
    "11. Thompson Sampling learning simulation\n",
    "12. Scheduler end-to-end dry-run\n",
    "13. Subset pytest execution\n",
    "14. Persist summary artifacts\n",
    "\n",
    "All network/API calls are stubbed or dry-run safe; no secrets required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup: Environment and Dependencies\n",
    "import os, sys, json, subprocess, textwrap, pathlib, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve().parent if Path('.').name == 'notebooks' else Path('.').resolve()\n",
    "SRC_DIR = PROJECT_ROOT / 'src'\n",
    "TESTS_DIR = PROJECT_ROOT / 'tests'\n",
    "\n",
    "# Ensure src is on PYTHONPATH\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "# Simulated dependency install (commented to avoid actual pip in notebook execution)\n",
    "print('[INFO] Environment prepared (simulated).')\n",
    "print('[INFO] PYTHONPATH entries:', sys.path[:3])\n",
    "\n",
    "# Placeholder .env content (not written for safety)\n",
    "sample_env = textwrap.dedent('''\\\n",
    "X_AUTH_MODE=tweepy\n",
    "X_API_KEY=FAKE\n",
    "X_API_KEY_SECRET=FAKE\n",
    "X_ACCESS_TOKEN=FAKE\n",
    "X_ACCESS_TOKEN_SECRET=FAKE\n",
    "ENABLE_TELEMETRY=false\n",
    "''')\n",
    "print(sample_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c35b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fetch Repository Tree (local enumeration fallback)\n",
    "import json, os\n",
    "repo_tree = []\n",
    "for root, dirs, files in os.walk(PROJECT_ROOT):\n",
    "    rel_root = os.path.relpath(root, PROJECT_ROOT)\n",
    "    if rel_root.startswith('.git') or rel_root.startswith('__pycache__'):\n",
    "        continue\n",
    "    for f in files:\n",
    "        repo_tree.append(os.path.join(rel_root, f))\n",
    "print('[INFO] Total files discovered:', len(repo_tree))\n",
    "print('[INFO] Sample:', repo_tree[:15])\n",
    "\n",
    "with open('repo_tree.json','w') as fh:\n",
    "    json.dump(repo_tree, fh, indent=2)\n",
    "print('[ARTIFACT] repo_tree.json written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Parse Python Modules and Map Architecture\n",
    "import ast, networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_module(path: Path):\n",
    "    try:\n",
    "        src = path.read_text(encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    try:\n",
    "        tree = ast.parse(src)\n",
    "    except SyntaxError:\n",
    "        return None\n",
    "    mod = {\n",
    "        'path': str(path),\n",
    "        'imports': [],\n",
    "        'defs': [],\n",
    "        'classes': [],\n",
    "    }\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Import):\n",
    "            for n in node.names:\n",
    "                mod['imports'].append(n.name.split('.')[0])\n",
    "        elif isinstance(node, ast.ImportFrom):\n",
    "            if node.module:\n",
    "                mod['imports'].append(node.module.split('.')[0])\n",
    "        elif isinstance(node, ast.FunctionDef):\n",
    "            mod['defs'].append(node.name)\n",
    "        elif isinstance(node, ast.ClassDef):\n",
    "            mod['classes'].append(node.name)\n",
    "    return mod\n",
    "\n",
    "modules = []\n",
    "for py in SRC_DIR.rglob('*.py'):\n",
    "    modules.append(analyze_module(py))\n",
    "modules = [m for m in modules if m]\n",
    "print('[INFO] Parsed modules:', len(modules))\n",
    "\n",
    "# Simple dependency graph\n",
    "G = nx.DiGraph()\n",
    "for m in modules:\n",
    "    mod_name = Path(m['path']).stem\n",
    "    G.add_node(mod_name)\n",
    "    for imp in m['imports']:\n",
    "        if imp in {'src','tests'}:\n",
    "            continue\n",
    "        G.add_edge(mod_name, imp)\n",
    "print('[INFO] Graph nodes:', len(G.nodes))\n",
    "print('[INFO] Graph edges:', len(G.edges))\n",
    "\n",
    "# Persist graph data\n",
    "graph_data = {'nodes': list(G.nodes), 'edges': list(G.edges)}\n",
    "with open('module_graph.json','w') as fh:\n",
    "    json.dump(graph_data, fh, indent=2)\n",
    "print('[ARTIFACT] module_graph.json written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b1884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Run ASCII and Formatting Checks (simulated)\n",
    "ascii_violations = []\n",
    "def is_ascii(s: str):\n",
    "    try:\n",
    "        s.encode('ascii')\n",
    "        return True\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "\n",
    "for py in SRC_DIR.rglob('*.py'):\n",
    "    content = py.read_text(encoding='utf-8', errors='ignore')\n",
    "    if not is_ascii(content):\n",
    "        ascii_violations.append(str(py))\n",
    "\n",
    "print('[INFO] ASCII violations:', ascii_violations if ascii_violations else 'None')\n",
    "# Ruff simulation (no real invocation here)\n",
    "ruff_result = {'summary':'Simulated check passed','issues':[]}\n",
    "print('[INFO] Ruff formatting/lint (simulated):', ruff_result)\n",
    "\n",
    "with open('lint_ascii_summary.json','w') as fh:\n",
    "    json.dump({'ascii_violations': ascii_violations,'ruff': ruff_result}, fh, indent=2)\n",
    "print('[ARTIFACT] lint_ascii_summary.json written')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceacb3e",
   "metadata": {},
   "source": [
    "## 6. Config Schema Validation\n",
    "\n",
    "Validate the main config file (`config.yaml`) against the strict schema in `src/config_schema.py`.\n",
    "- Check for required fields, types, and value constraints.\n",
    "- Summarize any errors or warnings.\n",
    "- Output: `config_validation_summary.json` (for downstream cells).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Config Schema Validation\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pydantic import ValidationError\n",
    "import sys\n",
    "sys.path.append(str(Path('src').resolve()))\n",
    "from config_schema import UnifiedConfig\n",
    "\n",
    "# Load config file\n",
    "config_path = Path('config.yaml')\n",
    "if not config_path.exists():\n",
    "    config_path = Path('config.example.yaml')\n",
    "\n",
    "try:\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        import yaml\n",
    "        config_data = yaml.safe_load(f)\n",
    "    config = UnifiedConfig(**config_data)\n",
    "    result = {'valid': True, 'errors': None}\n",
    "except (ValidationError, Exception) as e:\n",
    "    result = {'valid': False, 'errors': str(e)}\n",
    "\n",
    "with open('config_validation_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print('Config validation complete. See config_validation_summary.json.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb453bc9",
   "metadata": {},
   "source": [
    "## 7. Telemetry & Tracing Validation\n",
    "\n",
    "Check if OpenTelemetry tracing is enabled and functioning:\n",
    "- Inspect environment variables and config for telemetry settings.\n",
    "- Attempt to start a span using `telemetry.start_span`.\n",
    "- Summarize results and any errors.\n",
    "- Output: `telemetry_validation_summary.json`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d6744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Telemetry & Tracing Validation\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('src').resolve()))\n",
    "import telemetry\n",
    "\n",
    "telemetry_enabled = os.environ.get('ENABLE_TELEMETRY', 'false').lower() == 'true'\n",
    "span_result = None\n",
    "error = None\n",
    "try:\n",
    "    with telemetry.start_span('notebook-validation-span') as span:\n",
    "        span_result = str(span)\n",
    "except Exception as e:\n",
    "    error = str(e)\n",
    "\n",
    "result = {\n",
    "    'telemetry_enabled': telemetry_enabled,\n",
    "    'span_result': span_result,\n",
    "    'error': error\n",
    "}\n",
    "with open('telemetry_validation_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print('Telemetry validation complete. See telemetry_validation_summary.json.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ffbd65",
   "metadata": {},
   "source": [
    "## 8. Storage Validation\n",
    "\n",
    "Validate SQLite storage and key tables:\n",
    "- Check existence and schema of `data/agent_unified.db`.\n",
    "- Inspect action log, metrics, dedup tables, and bandit tables.\n",
    "- Summarize row counts and schema health.\n",
    "- Output: `storage_validation_summary.json`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ddfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Storage Validation\n",
    "import sqlite3\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "db_path = Path('data/agent_unified.db')\n",
    "result = {'db_exists': db_path.exists(), 'tables': {}, 'error': None}\n",
    "if db_path.exists():\n",
    "    try:\n",
    "        conn = sqlite3.connect(str(db_path))\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "        for table in tables:\n",
    "            cursor.execute(f\"PRAGMA table_info({table});\")\n",
    "            schema = cursor.fetchall()\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table};\")\n",
    "            row_count = cursor.fetchone()[0]\n",
    "            result['tables'][table] = {\n",
    "                'schema': schema,\n",
    "                'row_count': row_count\n",
    "            }\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "else:\n",
    "    result['error'] = 'Database file not found.'\n",
    "\n",
    "with open('storage_validation_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print('Storage validation complete. See storage_validation_summary.json.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe7783",
   "metadata": {},
   "source": [
    "## 9. Budget Validation\n",
    "\n",
    "Validate budget logic and safety buffers:\n",
    "- Check plan caps and buffer enforcement in `src/budget.py`.\n",
    "- Simulate a usage scenario and verify correct budget status.\n",
    "- Output: `budget_validation_summary.json`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Budget Validation\n",
    "import json\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('src').resolve()))\n",
    "import budget\n",
    "\n",
    "# Simulate usage scenario\n",
    "try:\n",
    "    # Example: free plan, 100 actions, 10MB storage\n",
    "    plan = 'free'\n",
    "    actions = 100\n",
    "    storage_mb = 10\n",
    "    status = budget.check_budget(plan=plan, actions=actions, storage_mb=storage_mb)\n",
    "    result = {'valid': True, 'status': status}\n",
    "except Exception as e:\n",
    "    result = {'valid': False, 'error': str(e)}\n",
    "\n",
    "with open('budget_validation_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print('Budget validation complete. See budget_validation_summary.json.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279b602",
   "metadata": {},
   "source": [
    "## 10. Rate Limiter Validation\n",
    "\n",
    "Validate per-endpoint rate limiting and backoff logic:\n",
    "- Simulate requests to key endpoints using `src/rate_limiter.py`.\n",
    "- Check backoff, jitter, and reset logic.\n",
    "- Output: `rate_limiter_validation_summary.json`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d74799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Rate Limiter Validation\n",
    "import json\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('src').resolve()))\n",
    "import rate_limiter\n",
    "\n",
    "result = {}\n",
    "try:\n",
    "    # Simulate requests to a test endpoint\n",
    "    endpoint = '/2/tweets'\n",
    "    rl = rate_limiter.RateLimiter()\n",
    "    # Simulate 5 requests\n",
    "    for i in range(5):\n",
    "        allowed, wait = rl.check(endpoint)\n",
    "        result[f'request_{i+1}'] = {'allowed': allowed, 'wait': wait}\n",
    "    result['valid'] = True\n",
    "except Exception as e:\n",
    "    result = {'valid': False, 'error': str(e)}\n",
    "\n",
    "with open('rate_limiter_validation_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print('Rate limiter validation complete. See rate_limiter_validation_summary.json.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd89f55",
   "metadata": {},
   "source": [
    "## 11. XClient Dry-Run Validation\n",
    "\n",
    "Run a dry-run using `src/x_client.py` and `src/main.py`:\n",
    "- Simulate posting and liking actions in both Tweepy and OAuth2 modes.\n",
    "- Ensure no real network calls are made and dry-run logic is respected.\n",
    "- Output: `xclient_dryrun_summary.json`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a241a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. XClient Dry-Run Validation\n",
    "import json\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('src').resolve()))\n",
    "from x_client import XClient\n",
    "from auth import UnifiedAuth\n",
    "\n",
    "results = {}\n",
    "for mode in ['tweepy', 'oauth2']:\n",
    "    try:\n",
    "        auth = UnifiedAuth(mode=mode)\n",
    "        client = XClient(auth=auth, dry_run=True)\n",
    "        post_result = client.post_tweet('Dry-run test tweet', media=None)\n",
    "        like_result = client.like_tweet('1234567890')\n",
    "        results[mode] = {\n",
    "            'post_result': str(post_result),\n",
    "            'like_result': str(like_result)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results[mode] = {'error': str(e)}\n",
    "\n",
    "with open('xclient_dryrun_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print('XClient dry-run validation complete. See xclient_dryrun_summary.json.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d26d3",
   "metadata": {},
   "source": [
    "## 12. Learning Loop Validation\n",
    "\n",
    "Validate Thompson Sampling and bandit updates:\n",
    "- Simulate arm selection and metric update using `src/learn.py`.\n",
    "- Output: `learning_validation_summary.json`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Learning Loop Validation\n",
    "import json\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('src').resolve()))\n",
    "import learn\n",
    "\n",
    "result = {}\n",
    "try:\n",
    "    # Simulate arm selection and metric update\n",
    "    arms = [('topic1', 'morning', 'image'), ('topic2', 'evening', 'none')]\n",
    "    selected = learn.select_arm(arms)\n",
    "    learn.update_metrics(selected, reward=1.0)\n",
    "    result = {'selected_arm': selected, 'update': 'success'}\n",
    "except Exception as e:\n",
    "    result = {'error': str(e)}\n",
    "\n",
    "with open('learning_validation_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print('Learning loop validation complete. See learning_validation_summary.json.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d714f2a",
   "metadata": {},
   "source": [
    "## 13. Scheduler Validation\n",
    "\n",
    "Validate time-window posting and orchestration:\n",
    "- Simulate scheduling actions using `src/scheduler.py`.\n",
    "- Output: `scheduler_validation_summary.json`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Scheduler Validation\n",
    "import json\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('src').resolve()))\n",
    "import scheduler\n",
    "\n",
    "result = {}\n",
    "try:\n",
    "    # Simulate scheduling a post for morning window\n",
    "    action = {'type': 'post', 'window': 'morning', 'content': 'Test scheduled tweet'}\n",
    "    scheduled = scheduler.schedule_action(action)\n",
    "    result = {'scheduled_action': scheduled}\n",
    "except Exception as e:\n",
    "    result = {'error': str(e)}\n",
    "\n",
    "with open('scheduler_validation_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print('Scheduler validation complete. See scheduler_validation_summary.json.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf91c70",
   "metadata": {},
   "source": [
    "## 14. Pytest Subset Validation\n",
    "\n",
    "Run a subset of unit tests for key modules:\n",
    "- Use `pytest` to run tests for `auth`, `budget`, `rate_limiter`, `storage`, and `x_client`.\n",
    "- Summarize results and failures.\n",
    "- Output: `pytest_subset_summary.json`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Pytest Subset Validation\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "test_files = [\n",
    "    'tests/test_auth.py',\n",
    "    'tests/test_budget.py',\n",
    "    'tests/test_rate_limiter.py',\n",
    "    'tests/test_storage.py',\n",
    "    'tests/test_x_client.py'\n",
    "]\n",
    "results = {}\n",
    "for test_file in test_files:\n",
    "    try:\n",
    "        proc = subprocess.run(['pytest', '-v', test_file], capture_output=True, text=True)\n",
    "        results[test_file] = {\n",
    "            'returncode': proc.returncode,\n",
    "            'stdout': proc.stdout,\n",
    "            'stderr': proc.stderr\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results[test_file] = {'error': str(e)}\n",
    "\n",
    "with open('pytest_subset_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print('Pytest subset validation complete. See pytest_subset_summary.json.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e11240",
   "metadata": {},
   "source": [
    "## 15. Artifact Export & Summary\n",
    "\n",
    "Export all validation summaries and key artifacts:\n",
    "- List all `*_summary.json` files generated in this notebook.\n",
    "- Provide a summary table of validation results.\n",
    "- Output: `notebook_artifact_index.json`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d57171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Artifact Export & Summary\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "artifact_files = list(Path('.').glob('*_summary.json'))\n",
    "index = {}\n",
    "for file in artifact_files:\n",
    "    try:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        index[file.name] = data\n",
    "    except Exception as e:\n",
    "        index[file.name] = {'error': str(e)}\n",
    "\n",
    "with open('notebook_artifact_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(index, f, indent=2)\n",
    "print('Artifact export complete. See notebook_artifact_index.json.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2421b",
   "metadata": {},
   "source": [
    "# Detailed Breakdown & Action Plan\n",
    "\n",
    "## 1. Improvement Opportunities\n",
    "\n",
    "**Modularization**\n",
    "- Identify and refactor tightly coupled logic in `x_client.py` and `actions.py`.\n",
    "- Separate API, business logic, and orchestration layers.\n",
    "- Add unit tests for new modules.\n",
    "\n",
    "**Telemetry Expansion**\n",
    "- Ensure all critical actions and errors are traced.\n",
    "- Add granular spans for key operations (auth, post, like, rate limit, storage).\n",
    "- Integrate OpenTelemetry context into logs.\n",
    "\n",
    "**Rate Limiter Integration**\n",
    "- Deepen integration of rate limiter with all client actions.\n",
    "- Log rate limit events and backoff decisions.\n",
    "- Add tests for edge cases and failures.\n",
    "\n",
    "**Security Hardening**\n",
    "- Review token handling and storage.\n",
    "- Audit error logging for sensitive data leaks.\n",
    "- Harden external API boundaries and input validation.\n",
    "\n",
    "**Learning Loop Refinement**\n",
    "- Tune Thompson Sampling parameters for better exploration/exploitation.\n",
    "- Add more robust bandit metrics and logging.\n",
    "- Test learning loop under varied scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Prioritization\n",
    "\n",
    "**Quick Wins**\n",
    "- Telemetry expansion\n",
    "- Rate limiter integration\n",
    "- Error logging improvements\n",
    "\n",
    "**Medium Effort**\n",
    "- Modularization\n",
    "- Security hardening\n",
    "\n",
    "**Strategic**\n",
    "- Learning loop enhancements\n",
    "- Advanced config validation\n",
    "- Multi-region storage\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Actionable Checklists\n",
    "\n",
    "### Modularization\n",
    "- [ ] List all tightly coupled functions/classes in `x_client.py` and `actions.py`\n",
    "- [ ] Refactor into separate modules\n",
    "- [ ] Add/expand unit tests\n",
    "- [ ] Document new module boundaries\n",
    "\n",
    "### Telemetry Expansion\n",
    "- [ ] Audit current tracing coverage\n",
    "- [ ] Add spans for missing operations\n",
    "- [ ] Integrate trace context into logs\n",
    "- [ ] Validate with OpenTelemetry collector\n",
    "\n",
    "### Rate Limiter Integration\n",
    "- [ ] Map all client actions to rate limiter logic\n",
    "- [ ] Add logging for rate limit events\n",
    "- [ ] Test backoff/jitter scenarios\n",
    "- [ ] Document integration points\n",
    "\n",
    "### Security Hardening\n",
    "- [ ] Review token handling/storage\n",
    "- [ ] Audit error logs for sensitive data\n",
    "- [ ] Harden API input validation\n",
    "- [ ] Add security-focused tests\n",
    "\n",
    "### Learning Loop Refinement\n",
    "- [ ] Review Thompson Sampling parameters\n",
    "- [ ] Add/expand bandit metrics\n",
    "- [ ] Test learning loop under edge cases\n",
    "- [ ] Document learning logic\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Metrics & Coverage\n",
    "\n",
    "Add a code cell to summarize test coverage, test counts, and risk hotspots by module using pytest and coverage data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15136900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Metrics & Coverage Summary\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Run pytest with coverage\n",
    "proc = subprocess.run(['pytest', '--cov=src', '--cov-report=json', '--maxfail=1', '--disable-warnings'], capture_output=True, text=True)\n",
    "coverage_file = Path('coverage.json')\n",
    "coverage_data = {}\n",
    "if coverage_file.exists():\n",
    "    with open(coverage_file, 'r', encoding='utf-8') as f:\n",
    "        coverage_data = json.load(f)\n",
    "else:\n",
    "    coverage_data = {'error': 'coverage.json not found'}\n",
    "\n",
    "# Summarize test counts and risk hotspots\n",
    "summary = {\n",
    "    'pytest_stdout': proc.stdout,\n",
    "    'pytest_stderr': proc.stderr,\n",
    "    'coverage': coverage_data\n",
    "}\n",
    "with open('metrics_coverage_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print('Metrics & coverage summary complete. See metrics_coverage_summary.json.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
